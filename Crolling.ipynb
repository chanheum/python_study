{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPTfylWr+PVbI+JRliBb/eM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chanheum/python_study/blob/main/Crolling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhNjOFAY8J8Z"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install bs4"
      ],
      "metadata": {
        "id": "SscQZbGH8aTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api"
      ],
      "metadata": {
        "id": "fsxxMJGyD7OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 네이버 검색 크롤링\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "query=\"페이커\"\n",
        "url = \"https://search.naver.com/search.naver?where=view&sm=tab_jum&query=\" + query\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "titles = soup.find_all(\"a\", class_=\"title_link\")\n",
        "# titles = soup.find_all(\"a\", class_=\"section_list_ranking_press \")\n",
        "\n",
        "for title in titles:\n",
        "    print(title.text)\n"
      ],
      "metadata": {
        "id": "sh3AfV9av4ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 네이버뉴스 크롤링\n",
        "url = \"https://news.naver.com/main/officeList.naver/\"\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200: # 정상 연결시\n",
        "    soup = BeautifulSoup(response.text, 'html.parser') # 수프 text 형태로 만들기\n",
        "    # post = soup.select('.list_text_inner a')\n",
        "    post = soup.select_one('#_rankingList0 > li:nth-child(2) > div > div > div > class')\n",
        "\n",
        "else:\n",
        "    print(response.status_code)\n",
        "\n",
        "print(post)"
      ],
      "metadata": {
        "id": "fJeWMAZo_FI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 네이버 증권 환율값 크롤링\n",
        "from bs4 import BeautifulSoup\n",
        "url = 'https://finance.naver.com/marketindex/'\n",
        "res = requests.get(url)\n",
        "soup = BeautifulSoup(res.text, 'html.parser')\n",
        "dollar = soup.select_one(\"#exchangeList > li.on > a.head.usd > div > span.value\").text\n",
        "print(\"미국달러 환율 : \", dollar)"
      ],
      "metadata": {
        "id": "t1hu3FQXvd3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글지도 크롤링\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "query=\"부천\"\n",
        "\n",
        "url = 'https://search.naver.com/search.naver?sm=tab_hty.top&where=nexearch&query=%EB%B2%94%EC%95%88%EB%8F%99+%EB%82%A0%EC%94%A8&oquery=%EB%B2%94%EC%95%88%EB%8F%99&tqi=ihzu1dqo1fssslo0PxKssssss4w-250477'\n",
        "res = requests.get(url)\n",
        "soup = BeautifulSoup(res.text, 'html.parser')\n",
        "# star = soup.find_all(\"dl\", {\"class\":\"summary_list\"})[0].text    # find_all or find + (태그, 클래스명)\n",
        "\n",
        "data = soup.select_one('span')\n",
        "print(\"태그이름만 특정 : \", data)\n",
        "\n",
        "data = soup.select_one('.temperature_info')\n",
        "print(\"태그 class 특정 : \", data)\n",
        "\n",
        "star = soup.select_one('span.temperature')\n",
        "print(\"태그이름과 class 모두 특정 : \", data)\n",
        "\n",
        "star = soup.select_one('#summary')\n",
        "print(\"태그ID 특정 : \", data)\n",
        "\n",
        "star = soup.select_one('dl#summary')\n",
        "print(\"태그 이름과 ID 모두 특정 : \", data)"
      ],
      "metadata": {
        "id": "b90js8vwJznn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 네이버블로그\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = 'https://www.op.gg/summoners/kr/Radiohead-KR1'\n",
        "response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# data = soup.select('#content-container > div.css-18w3o0f.eptxecf0 > div > div.champion-box > div.info > div.name > a')\n",
        "most_champs = soup.select('div.css-18w3o0f.eptxecf0 > div > div.champion-box > div.info > div.name > a')\n",
        "\n",
        "for champ in most_champs:\n",
        "    print(champ.text)"
      ],
      "metadata": {
        "id": "pbEFwdPOC6A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위키피디아 크롤링\n",
        "import wikipediaapi #사용할 api 호출\n",
        "wiki = wikipediaapi.Wikipedia('Crolling','ko') #한국 wikipedia 사이트로 접속하도록 셋팅하기\n",
        "\n",
        "search_data = '수탉'\n",
        "page_py = wiki.page(search_data)\n",
        "\n",
        "if(page_py.exists()):\n",
        "    print(\"Page - Title: %s\" % page_py.title)\n",
        "    print(\"Page - Summary: %s\" % page_py.summary)\n",
        "    print(\"Page - Text: \\n%s\" % page_py.text)\n",
        "else:\n",
        "    print(\"데이터가 존재하지 않습니다.\")\n"
      ],
      "metadata": {
        "id": "feS774t5EIhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# # resp = requests.get('https://finance.naver.com/')\n",
        "# # resp = requests.get('https://blog.naver.com/kspak56/223303178383')\n",
        "# resp = requests.get('https://search.naver.com/search.naver?nso=&page=2&query=%EA%B5%AD%EB%82%B4+%EA%B0%80%EB%B3%BC%EB%A7%8C%ED%95%9C%EA%B3%B3&sm=tab_pge&start=1&where=web')\n",
        "# html = resp.text\n",
        "# soup = BeautifulSoup(html, 'html.parser')\n",
        "# # news = soup.select('.news_area a')\n",
        "# post = soup.select('.api_subject_bx a')\n",
        "\n",
        "# # news\n",
        "# post\n",
        "\n",
        "# url = 'https://blog.naver.com/xoxo_pch'\n",
        "# url = 'https://finance.naver.com/'\n",
        "url = \"https://news.naver.com/main/officeList.naver\"\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200: # 정상 연결시\n",
        "    soup = BeautifulSoup(response.text, 'html.parser') # 수프 text 형태로 만들기\n",
        "    post = soup.select('.section._officeTopRanking1087479 a')\n",
        "else:\n",
        "    print(response.status_code)\n",
        "\n",
        "post"
      ],
      "metadata": {
        "id": "6l0WJ-rQ874C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = []\n",
        "url = []\n",
        "\n",
        "for n in news:\n",
        "    title.append(n.text)\n",
        "    url.append(n['href'])\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['제목'] = title\n",
        "df['url'] = url\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "hS0n3xve9ryx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Ccu45Vx9uep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}